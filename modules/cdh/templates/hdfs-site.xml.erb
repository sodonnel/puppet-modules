<?xml version="1.0"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

  <!-- This should be a list of directories in a real cluster
       one of which should be an NFS mount if NN HA is not setup.
       The mount options should be 'tcp,soft,intr,timeo=10,retrans=10' -->
  <property>
     <name>dfs.namenode.name.dir</name>
     <value>file:///var/lib/hadoop-hdfs/cache/hdfs/dfs/name</value>
  </property>

  <property>
     <name>dfs.namenode.checkpoint.dir</name>
     <value>file:///var/lib/hadoop-hdfs/cache/hdfs/dfs/secondname</value>
  </property>

  <!-- default is 3600 seconds, ie 1 hour -->
  <property>
     <name>dfs.namenode.checkpoint.period</name>
     <value>3600</value>
  </property>
  
  <property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///data/1/dfs/dn</value>
  </property>

  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>hadoop</value>
  </property>

  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>

<!-- Settings useful for Impala -->

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn._PORT</value>
  </property>

  <property>
    <name>dfs.client.file-block-storage-locations.timeout.millis</name>
    <value>10000</value>
  </property>

  <!-- This exposes the disk on a datanode on which a block resides -->
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>

<!-- end impala -->

<!-- Recommendations  -->
  
  <!-- recommend 25% of space on each volume with a min of 10GB
       value is in bytes
  <property>
     <name>dfs.datanode.du.reserved</name>
     <value></value>
   </property>
  -->  

  <!-- This is a client setting. Default used to be 64MB, is not 128MB -->
  <property>
     <name>dfs.blocksize</name>
     <value>1134217728</value>
  </property>

  <!-- This is a client setting. Default is 3
  <property>
     <name>dfs.replication</name>
     <value>3</value>
  </property>
  -->

  <!-- Number of threads namenode uses to handle requests from datanodes.
       Recommend ln(num cluster nodes) * 20
       If set too low datanodes will report 'connection refused' as they
       try to send blocks reports -->
  <property>
     <name>dfs.namenode.handler.count</name>
     <value>10</value>
  </property>

  <!-- Num Volumes allowed to fail before DataNode goes offline.
       Default is zero but increase if machine has multiple disks -->
  <property>
     <name>dfs.datanode.failed.volumes.tolerated</name>
     <value>0</value>
  </property>

  <!-- Max memory used for caching ina data node. Need to set a limit in
       /etc/security/limits.conf for RLIMIT_MEMLOCK. ulimit -l should report the value -->
  <property>
     <name>dfs.datanode.max.locked.memory</name>
     <value>0</value>
  </property>


  <!-- File specifying hosts allowed to connect as datanodes -->
  <!--
  <property>
     <name>dfs.hosts</name>
     <value>/etc/hadoop/conf/allowedhosts</value>
   </property>
   -->  

  <!-- File specifying hosts not allowed to connect. Good for decomissioning a node -->
  <!--
  <property>
     <name>dfs.hosts.exclude</name>
     <value>/etc/hadoop/conf/excludehosts</value>
  </property>
  -->


    
  

<!-- END Recommendations -->


  <% if @secure == true %>
  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.namenode.keytab.file</name>
    <value>/etc/hadoop/conf/hdfs.keytab</value>
  </property>

  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@EXAMPLE.COM</value>
  </property>

  <property>
    <name>dfs.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@EXAMPLE.COM</value>
  </property>

  <property>
    <name>dfs.secondary.namenode.keytab.file</name>
    <value>/etc/hadoop/conf/hdfs.keytab</value>
  </property>

  <property>
    <name>dfs.secondary.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@EXAMPLE.COM</value>
  </property>

  <property>
    <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@EXAMPLE.COM</value>
  </property>  

  <property>
    <name>dfs.datanode.keytab.file</name>
    <value>/etc/hadoop/conf/hdfs.keytab</value>
  </property>

  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>hdfs/_HOST@EXAMPLE.COM</value>
  </property>

  <property>
    <name>dfs.web.authentication.kerberos.keytab</name>
    <value>/etc/hadoop/conf/hdfs.keytab</value>
  </property>

  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@EXAMPLE.COM</value>
  </property>

  <!-- secure ports for datanode under security -->
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:1004</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:1006</value>
  </property>

  <% if @encryption == true %>

  <property>
    <name>dfs.https.enable</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>

  <property>
    <name>dfs.namenode.https-address.mycluster.nn1</name>
    <value>standalone:50470</value>
  </property>

  <!-- With encryption enabled, this does not need to be
       on a priv port, so put it back to the default 50010 -->
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:50010</value>
  </property>

  <property>
    <name>dfs.datanode.https.address</name>
    <value>standalone:50475</value>
  </property>


  <!-- The next 4 are for data transport encryption -->

  <!-- If this is not set, you get an error about the DN being unable to start on
       non priv ports unless HTTPS and SASL protection are on. This seems to need to
       be set with or without dfs.encrypt.data.transfer although the docs suggest
       dfs.encrypt.data.transfer override that setting -->
  <property>
    <name>dfs.data.transfer.protection</name>
    <value>privacy</value>
  </property>
  
  <property>
    <name>dfs.encrypt.data.transfer</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.encrypt.data.transfer.cipher.suites</name>
    <value>AES/CTR/NoPadding</value>
  </property>

  <!-- Ideally this should be 256, but it needs the Java unlimited strength encryption
       libraries installed -->
  <property>
    <name>dfs.encrypt.data.transfer.cipher.key.bitlength</name>
    <value>128</value>
  </property>
  
  
  <% end %>
  
  <% end %>


</configuration>
